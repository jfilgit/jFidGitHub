{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2a939da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Commercial banks receive a lot of applications for credit cards. \n",
    "#Many of them get rejected for many reasons, like high loan balances, low income levels, \n",
    "#or too many inquiries on an individual's credit report, for example. \n",
    "#Manually analyzing these applications is mundane, error-prone, and time-consuming (and time is money!). \n",
    "#Luckily, this task can be automated with the power of machine learning and pretty much every commercial bank does so nowadays. \n",
    "#In this notebook, we will build an automatic credit card approval predictor using machine learning techniques, \n",
    "#just like the real banks do.\n",
    "\n",
    "\n",
    "# Import pandas\n",
    "#import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "#cc_apps = pd.read_csv(\"datasets/cc_approvals.data\", header=None)\n",
    "\n",
    "# Inspect data\n",
    "#cc_apps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77bd5a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Print summary statistics\n",
    "#cc_apps_description = cc_apps.describe()\n",
    "#print(cc_apps_description)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Print DataFrame information\n",
    "#cc_apps_info = cc_apps.info()\n",
    "#print(cc_apps_info)\n",
    "\n",
    "#print('\\n')\n",
    "\n",
    "# Inspect missing values in the dataset\n",
    "#cc_apps.tail(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfd870c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Drop the features 11 and 13\n",
    "#cc_apps = cc_apps.drop([11, 13], axis=1)\n",
    "\n",
    "# Split into train and test sets\n",
    "#cc_apps_train, cc_apps_test = train_test_split(cc_apps, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0108b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy\n",
    "#import numpy as np\n",
    "\n",
    "# Replace the '?'s with NaN in the train and test sets\n",
    "#cc_apps_train = cc_apps_train.replace('?', np.NaN)\n",
    "#cc_apps_test = cc_apps_test.replace('?', np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "946723a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the missing values with mean imputation\n",
    "#cc_apps_train.fillna(cc_apps_train.mean(), inplace=True)\n",
    "#cc_apps_test.fillna(cc_apps_train.mean(), inplace=True)\n",
    "\n",
    "# Count the number of NaNs in the datasets and print the counts to verify\n",
    "#print(cc_apps_train.isnull().sum())\n",
    "#print(cc_apps_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07584381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each column of cc_apps_train\n",
    "#for col in cc_apps_train.columns:\n",
    "    # Check if the column is of object type\n",
    "#    if cc_apps_train[col].dtypes == 'object':\n",
    "        # Impute with the most frequent value\n",
    "#        cc_apps_train = cc_apps_train.fillna(cc_apps_train[col].value_counts().index[0])\n",
    "#        cc_apps_test = cc_apps_test.fillna(cc_apps_train[col].value_counts().index[0])\n",
    "\n",
    "# Count the number of NaNs in the dataset and print the counts to verify\n",
    "#print(cc_apps_train.isnull().sum())\n",
    "#print(cc_apps_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c00122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the categorical features in the train and test sets independently\n",
    "#cc_apps_train = pd.get_dummies(cc_apps_train)\n",
    "#cc_apps_test = pd.get_dummies(cc_apps_test)\n",
    "\n",
    "# Reindex the columns of the test set aligning with the train set\n",
    "#cc_apps_test = cc_apps_test.reindex(columns=cc_apps_train.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0493c80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MinMaxScaler\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Segregate features and labels into separate variables\n",
    "#X_train, y_train = cc_apps_train.iloc[:, :-1].values, cc_apps_train.iloc[:, [-1]].values\n",
    "#X_test, y_test = cc_apps_test.iloc[:, :-1].values, cc_apps_test.iloc[:, [-1]].values\n",
    "\n",
    "# Instantiate MinMaxScaler and use it to rescale X_train and X_test\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#rescaledX_train = scaler.fit_transform(X_train)\n",
    "#rescaledX_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "553efe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which model should we pick? \n",
    "#A question to ask is: are the features that affect the credit card approval decision process correlated with each other? \n",
    "#Although we can measure correlation, that is outside the scope of this notebook, \n",
    "#so we'll rely on our intuition that they indeed are correlated for now. \n",
    "#Because of this correlation, we'll take advantage of the fact that generalized linear models perform well in these cases. \n",
    "#Let's start our machine learning modeling with a Logistic Regression model (a generalized linear model).\n",
    "\n",
    "# Import LogisticRegression\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate a LogisticRegression classifier with default parameter values\n",
    "#logreg = LogisticRegression()\n",
    "\n",
    "# Fit logreg to the train set\n",
    "#logreg.fit(rescaledX_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "929eddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#But how well does our model perform?\n",
    "#We will now evaluate our model on the test set with respect to classification accuracy. \n",
    "#But we will also take a look the model's confusion matrix. \n",
    "#In the case of predicting credit card applications, \n",
    "#it is important to see if our machine learning model is equally capable of predicting approved and denied status, \n",
    "#in line with the frequency of these labels in our original dataset. \n",
    "#If our model is not performing well in this aspect, \n",
    "#then it might end up approving the application that should have been approved. \n",
    "#The confusion matrix helps us to view our model's performance from these aspects.\n",
    "\n",
    "\n",
    "# Import confusion_matrix\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Use logreg to predict instances from the test set and store it\n",
    "#y_pred = logreg.predict(rescaledX_test)\n",
    "\n",
    "# Get the accuracy score of logreg model and print it\n",
    "#print(\"Accuracy of logistic regression classifier: \", logreg.score(rescaledX_test,y_test))\n",
    "\n",
    "# Print the confusion matrix of the logreg model\n",
    "#confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d89f73df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our model was pretty good! In fact it was able to yield an accuracy score of 100%.\n",
    "#For the confusion matrix, \n",
    "#the first element of the of the first row of the confusion matrix denotes the true negatives \n",
    "#meaning the number of negative instances (denied applications) predicted by the model correctly. \n",
    "#And the last element of the second row of the confusion matrix denotes the true positives \n",
    "#meaning the number of positive instances (approved applications) predicted by the model correctly.\n",
    "#But if we hadn't got a perfect score what's to be done?. \n",
    "#We can perform a grid search of the model parameters to improve the model's ability to predict credit card approvals.\n",
    "#scikit-learn's implementation of logistic regression consists of different hyperparameters but we will grid search over the following two:\n",
    "#tol\n",
    "#max_iter\n",
    "\n",
    "# Import GridSearchCV\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the grid of values for tol and max_iter\n",
    "#tol = [0.01, 0.001 ,0.0001]\n",
    "#max_iter = [100, 150, 200]\n",
    "\n",
    "# Create a dictionary where tol and max_iter are keys and the lists of their values are the corresponding values\n",
    "#param_grid = dict(tol=tol, max_iter=max_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c4dae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have defined the grid of hyperparameter values and converted them into a single dictionary format which GridSearchCV() expects as one of its parameters. \n",
    "#Now, we will begin the grid search to see which values perform best.\n",
    "#We will instantiate GridSearchCV() with our earlier logreg model with all the data we have. \n",
    "#We will also instruct GridSearchCV() to perform a cross-validation of five folds.\n",
    "#We'll end the notebook by storing the best-achieved score and the respective best parameters.\n",
    "\n",
    "# Instantiate GridSearchCV with the required parameters\n",
    "#grid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit grid_model to the data\n",
    "#grid_model_result = grid_model.fit(rescaledX_train, y_train)\n",
    "\n",
    "# Summarize results\n",
    "#best_score, best_params = grid_model_result.best_score_, grid_model_result.best_params_\n",
    "#print(\"Best: %f using %s\" % (best_score, best_params))\n",
    "\n",
    "# Extract the best model and evaluate it on the test set\n",
    "#best_model = grid_model_result.best_estimator_\n",
    "#print(\"Accuracy of logistic regression classifier: \", best_model.score(rescaledX_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2071049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
